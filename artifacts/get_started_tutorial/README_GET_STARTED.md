# Get Start Tutorial: Compile a NASRNN model with Cocktailer
We assume you already build and install Cocktailer following the *Environment Preparation* section in [README.md](../README.md).

The goal of this tutorial is to demonstrate how to compile and optimize a typical DNN model with control flow, and showcase the performance improvement with Cocktailer compiler.

## Run PyTorch, TensorFlow, and JAX baselines

```bash
export ARTIFACT_ROOT=$YOUR_DIR_FOR_NNFUSION/nnfusion/artifacts
cd $ARTIFACT_ROOT/get_started_tutorial
source ~/miniconda3/etc/profile.d/conda.sh

# PyTorch
conda activate controlflow
python3 nasrnn_pytorch.py --bs 64
conda deactivate

# TensorFlow
conda activate baseline_tf1
python3 nasrnn_tf.py --bs 64
conda deactivate

# JAX
conda activate baseline_jax
python3 nasrnn_jax.py --bs 64
conda deactivate
```

The output should be similar to the following:
```bash
# PyTorch with TorchScript enabled
Summary: [min, max, mean] = [107.626438, 110.256910, 108.871682] ms

# TensorFlow
Summary: [min, max, mean] = [297.036409, 335.924387, 323.820636] ms

# JAX with JIT enabled
Summary: [min, max, mean] = [43.358564, 43.553829, 43.469448] ms
```

## Run CocktailerBase and Cocktailer

## Prepare kernel database

Cocktailer needs the source code of dataflow operators to generate the optimized code. The source code of operators in NASRNN includes BatchMatMul generated by Roller, and built-in element-wise operators in [NNFusion](https://github.com/microsoft/nnfusion/tree/main/src/nnfusion/core/kernels/cuda_gpu/kernels). (TODO: check branch) Below is the script to generate and save the BatchMatmul kernel.

```bash
export ARTIFACT_ROOT=$YOUR_DIR_FOR_NNFUSION/nnfusion/artifacts
cd $ARTIFACT_ROOT/kernel_db
source ~/miniconda3/etc/profile.d/conda.sh
conda activate kerneldb
python3 example_roller_kernels.py --reproduce
bash -c "mkdir -p /tmp/`whoami` && cp -r ~/.cache/nnfusion/* /tmp/`whoami`/"
conda deactivate
cd $ARTIFACT_ROOT/get_started_tutorial
```

After that, you can get a kernel database file in `~/.cache/nnfusion/kernel_cache.db`. NNFusion will automatically detect this path and import these kernels.

## Run CocktailerBase
```bash
export ARTIFACT_ROOT=$YOUR_DIR_FOR_NNFUSION/nnfusion/artifacts
cd $ARTIFACT_ROOT/get_started_tutorial
source ~/miniconda3/etc/profile.d/conda.sh
conda activate controlflow

python3 nasrnn.py --platform V100 --bs 64 --no-torch --disable-cf --measure
```

The output should be similar to the following:
```bash
......

def forward(self, inputs):
    state_c = torch.zeros(64, 256, device='cuda')
    state_m = torch.zeros(64, 256, device='cuda')
    for i in range(1000):
        (state_m, state_c) = self.__base_nasrnn_bs64_0.apply(state_m, inputs, i, state_c)
    state = self.__base_nasrnn_bs64_2.apply(state_m, state_c)
    return state

......
tensor equals!
......
100 iters, min = 65.4466 ms, max = 66.7112 ms, avg = 65.8735 ms
```

The `forward` function in the output is the python code executed during time measurement, which accelerates the basic block of the model (locate at `/dev/shm/$USER/controlflow/base_nasrnn_bs64_0/forward` and `/dev/shm/$USER/controlflow/base_nasrnn_bs64_2/forward`) and relies on PyTorch for executing the control flows. The `tensor equals!` indicates that the output of CocktailerBase matches that of PyTorch.

## Run Cocktailer
```bash
export ARTIFACT_ROOT=$YOUR_DIR_FOR_NNFUSION/nnfusion/artifacts
cd $ARTIFACT_ROOT/get_started_tutorial
source ~/miniconda3/etc/profile.d/conda.sh
conda activate controlflow

python3 nasrnn.py --platform V100 --bs 64 --no-torch --measure
```
The output should be similar to the following:
```bash
......
def forward(self, inputs):
    state = self.__nasrnn_bs64_0.apply(inputs)
    return state
......
Best flag for nasrnn_bs64_0 is 6 (-f onnx -fcodegen_unexist_kernel=true -fproduct_name=V100 -fbiasadd_fix=true -fcheck_result=true -fextern_result_memory=true -fconv_cnhw=false -fdefault_device=CUDA -fkernel_cache_path=/tmp/******/kernel_cache.db -fcf_level=1 -fmax_grid_dim=320 -fmax_block_dim=128)
......
tensor equals!
......
100 iters, min = 25.3108 ms, max = 25.7773 ms, avg = 25.3788 ms
```

The generated code of Cocktailer is located at `/dev/shm/$USER/controlflow/nasrnn_bs64_0/forward`. The `Best flag` indicates the schedule result of Cocktailer. The `tensor equals!` indicates that the output of Cocktailer matches that of PyTorch.

## Summary
The following table summarizes the above experiments. Cocktailer achieves $1.71\times$ speedup against the fastest baseline (JAX).

|             | TorchScript | TensorFlow | JAX | CocktailerBase | Cocktailer |
|:-----------:|:------:|:--:|:----:|:--:|:---:|
| **Time (ms)**   |    108.87    |  323.82 |   43.47  |  65.87 |  25.38  |
